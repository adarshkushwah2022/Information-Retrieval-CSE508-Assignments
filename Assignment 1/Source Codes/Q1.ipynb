{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR ASSIGNMENT 1 Q1 Final Done.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-xvMRoJR9RH",
        "outputId": "e89c8cbb-a0f9-41a3-81e2-931d301cbefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#Importing and downloading required libraries\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import PorterStemmer\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3nEg4Nm8N4kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf980552-a9a3-4e9c-f0a8-569c5137270b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "id": "bqhxqIHoSYr_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required files\n",
        "datasetFiles = os.listdir('/content/drive/MyDrive/Assignments/IR Assigment 1/Humor,Hist,Media,Food')\n",
        "print(color.BOLD+color.BLUE+\"Number of files in Dataset folder:\"+color.END,len(datasetFiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBmi-RscSj_D",
        "outputId": "2dd12f72-fe3e-4597-f72d-64682e1e5b05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94mNumber of files in Dataset folder:\u001b[0m 1133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing dataset files name\n",
        "datasetFiles[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft-vitPcSmZE",
        "outputId": "d2a80210-c11f-430e-9579-9e81af8ea574"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bbq.txt',\n",
              " 'grommet.hum',\n",
              " 'mowers.txt',\n",
              " 'hell.jok',\n",
              " 'harmful.hum',\n",
              " 'herb!.hum',\n",
              " 'hate.hum',\n",
              " 'mothers.txt',\n",
              " 'hi.tec',\n",
              " 'murph.jok',\n",
              " 'conan.txt',\n",
              " 'icm.hum',\n",
              " 'howlong.hum',\n",
              " 'roach.asc',\n",
              " 'imprrisk.hum',\n",
              " 'coyote.txt',\n",
              " 'horoscop.jok',\n",
              " 'insanity.hum',\n",
              " 'dirtword.txt',\n",
              " 'bible.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating separate dictionaries for storing file names along with thier location path\n",
        "nameId = {}\n",
        "locationId = {} \n",
        "nameDictionary = {} \n",
        "for iterator in range(len(datasetFiles)):\n",
        "    nameDictionary[datasetFiles[iterator]] = iterator\n",
        "    nameId[iterator] = datasetFiles[iterator] \n",
        "    locationId[iterator] = '/content/drive/MyDrive/Assignments/IR Assigment 1/Humor,Hist,Media,Food/' + datasetFiles[iterator]"
      ],
      "metadata": {
        "id": "ngDUUAmnSvC1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dictionaries and list for storing posting lists, frequency, corpus, unique words\n",
        "uniqueWords = []\n",
        "postingsList = {}         \n",
        "datasetCorpus = {} \n",
        "frequencyDictionary = {}           \n",
        "postingsListDictionary = {}  "
      ],
      "metadata": {
        "id": "EbZf7_VQSx-M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing lemmatizer object\n",
        "preProcessLemmatizer = WordNetLemmatizer() "
      ],
      "metadata": {
        "id": "VPKMIKoHS8rw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating dataset corpus....\")\n",
        "for fileNumber in range(len(datasetFiles)):\n",
        "\n",
        "    currentFileLocation = locationId[fileNumber]\n",
        "    currentFile = open(currentFileLocation, 'r', encoding = 'utf-8', errors = 'ignore')\n",
        "    currentFileText = currentFile.read()\n",
        "\n",
        "    currentFileText = currentFileText.lower()                                                                    #Conversion to lower\n",
        "\n",
        "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''                                                                   #Removing punctuations\n",
        "    for ele in currentFileText:               \n",
        "      if ele in punc:  \n",
        "        currentFileText = currentFileText.replace(ele, \" \")\n",
        "\n",
        "    currentFileText = re.sub(r'[\\!@#$%^&\\()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']',\"\",currentFileText)                         #Removing Junk symbols\n",
        "\n",
        "    tokens = word_tokenize(currentFileText)                                                                      #Tokenization\n",
        "\n",
        "    tokens = [preProcessLemmatizer.lemmatize(word) for word in tokens]                                           #Lemmatization\n",
        "\n",
        "    tokensWithoutStopwords= [word for word in tokens if not word in stopwords.words('english') and len(word)>1]  #Removing Stopwords\n",
        "\n",
        "    textFetched = ' '.join(tokensWithoutStopwords)                                                               #Converting to single string\n",
        "    \n",
        "    datasetCorpus[datasetFiles[fileNumber]]=textFetched.split()                                                  #Saving to dataCourpus dictionary\n",
        "\n",
        "    for singleWord in tokensWithoutStopwords:                                                                    #Storing frequency of words of file\n",
        "        if(singleWord in frequencyDictionary.keys()):\n",
        "            frequencyDictionary[singleWord] = frequencyDictionary[singleWord] + 1\n",
        "        else:\n",
        "            frequencyDictionary[singleWord] = 1\n",
        "            uniqueWords.append(singleWord)\n",
        "print(\"Dataset corpus created successfully.\")"
      ],
      "metadata": {
        "id": "KyN751MFTBIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d51d1d-63e4-4174-f043-ab155ce0a466"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset corpus....\n",
            "Dataset corpus created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(color.BOLD+\"Some Dataset Corpus Instances:\"+color.END)\n",
        "print(color.BOLD+\"File name\\t  Words present in file text\"+color.END)\n",
        "print(datasetFiles[nameDictionary['bbq.txt']],'\\t',datasetCorpus['bbq.txt'])\n",
        "print(datasetFiles[nameDictionary['rinaldos.txt']],'\\t',datasetCorpus['rinaldos.txt'])\n",
        "print(datasetFiles[nameDictionary['llong.hum']],'\\t',datasetCorpus['llong.hum'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3StmYPKeQqpW",
        "outputId": "966bf818-75ae-483e-81f2-2c795ec3322c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mSome Dataset Corpus Instances:\u001b[0m\n",
            "\u001b[1mFile name\t  Words present in file text\u001b[0m\n",
            "bbq.txt \t ['agreement', 'participation', 'unetix', 'post', 'air', 'show', 'bbq', 'unetix', 'incorporated', 'hereinafter', 'host', 'print', 'full', 'name', 'hereinafter', 'guest', 'whereas', 'host', 'desire', 'make', 'available', 'certain', 'facility', 'foodstuff', 'specified', 'article', 'barbecue', 'format', 'hereinafter', 'event', 'guest', 'prepared', 'attend', 'said', 'facility', 'consume', 'said', 'foodstuff', 'specified', 'article', 'agreement', 'consideration', 'mutual', 'promise', 'contained', 'herein', 'host', 'agreement', 'hold', 'event', 'guest', 'agreement', 'attend', 'event', 'term', 'agreement', 'subject', 'term', 'condition', 'contained', 'herein', 'party', 'agree', 'follows', 'article', 'host', 'agrees', 'provide', 'facility', 'suitable', 'event', 'specifically', 'following', 'facility', 'ha', 'chosen', 'hereinafter', 'facility', 'insert', 'address', 'bbq', 'following', 'paragraph', 'construed', 'direction', 'may', 'assistance', 'locating', 'facility', 'insert', 'direction', 'bbq', 'host', 'agrees', 'provide', 'beverage', 'snack', 'type', 'food', 'specifically', 'may', 'include', 'none', 'following', 'beer', 'soft', 'drink', 'calistogas', 'potato', 'chip', 'tortilla', 'chip', 'vegetable', 'inclusion', 'list', 'doe', 'imply', 'availability', 'host', 'shall', 'make', 'available', 'guest', 'grilling', 'device', 'suitable', 'barbecue', 'process', 'temperature', 'grill', 'shall', 'sufficiently', 'hot', 'char', 'surface', 'make', 'direct', 'contact', 'grill', 'hot', 'char', 'surface', 'directly', 'contact', 'grill', 'unless', 'surface', 'allowed', 'remain', 'grill', 'excessive', 'period', 'determined', 'host', 'article', 'guest', 'attendance', 'facility', 'term', 'agreement', 'implies', 'guest', 'participation', 'event', 'guest', 'may', 'invite', 'additional', 'guest', 'guest', 'advised', 'bring', 'item', 'suitable', 'grilling', 'guest', 'lacking', 'item', 'suitable', 'grilling', 'welcome', 'event', 'case', 'host', 'assumes', 'responsibility', 'guest', 'participation', 'barbecue', 'process', 'way', 'implies', 'host', 'assumes', 'responsibility', 'anything', 'whatsoever', 'circumstance', 'article', 'agreement', 'shall', 'full', 'force', 'effect', 'commencing', '00', 'saturday', 'july', '1988', 'moffett', 'air', 'show', 'terminates', 'mutual', 'agreement', 'host', 'guest', 'exclusive', 'discretion', 'host', 'whichever', 'occurs', 'first', 'article', 'event', 'offered', 'without', 'warranty', 'kind', 'either', 'expressed', 'implied', 'including', 'limited', 'implied', 'warranty', 'merchantability', 'fitness', 'particular', 'purpose', 'article', 'witness', 'whereof', 'party', 'hereto', 'executed', 'agreement', 'date', 'noted', 'unetix', 'incorporated', 'date', 'guest', 'date']\n",
            "rinaldos.txt \t ['rinaldo', 'law', 'leaving', 'washington', 'area', 'early', 'may', 'thought', 'appropriate', 'share', 'wisdom', 'accumulated', 'thus', 'far', 'truth', 'come', 'vision', 'observation', 'time', 'accordingly', 'synthesized', 'following', 'law', 'choreography', 'reward', 'thing', 'done', 'sake', 'form', 'fight', 'looking', 'substance', 'everything', 'long', 'enough', 'find', 'enjoyment', 'elephant', 'dance', 'doe', 'work', 'shape', 'applied', 'computer', 'writes', 'code', 'rule', 'coding', 'rule', 'meeting', 'writes', 'minute', 'determines', 'outcome', 'le', 'knowledge', 'jealously', 'preserved', 'society', 'precious', 'fact', 'make', 'people', 'memorize', 'pledge', 'faithfully', 'abide', 'contrast', 'highly', 'developed', 'discipline', 'quit', 'worrying', 'losing', 'knowledge', 'unless', 'computer', 'crash', 'backup', 'excellence', 'increase', 'demand', 'critic', 'gather', 'spot', 'tinier', 'flaw', 'work', 'nears', 'perfection', 'promptness', 'invite', 'impatience', 'correspondence', 'faster', 'answer', 'letter', 'faster', 'correspondent', 'answer', 'giving', 'something', 'shorter', 'deadline', 'reach', 'fever', 'pitch', 'electronic', 'mail', 'skill', 'diminish', 'professionalism', 'engineer', 'admit', 'drafting', 'skill', 'vulnerable', 'assignment', 'drafting', 'work', 'help', 'similarly', 'female', 'professional', 'hide', 'clerical', 'skill', 'lest', 'asked', 'pinch', 'hit', 'one', 'secretary', 'event', 'illness', 'separate', 'competent', 'incompetent', 'ability', 'cover', 'mistake', 'many', 'successful', 'sale', 'demonstration', 'made', 'defective', 'product', 'hand', 'competent', 'person', 'avoid', 'demonstrating', 'feature', 'work', 'beautiful', 'xerox', 'copy', 'made', 'original', 'riddled', 'correction', 'fluid', 'recovery', 'grievous', 'error', 'attained', 'simply', 'announcing', 'problem', 'put', 'back', 'word', 'processor', 'computer', 'software', 'profession', 'seems', 'exception', 'else', 'blatant', 'term', 'debugging', 'let', 'world', 'know', 'need', 'extra', 'time', 'funded', 'customer', 'correct', 'error', 'silence', 'acquiescence', 'contrary', 'may', 'heard', 'silence', 'present', 'necessarily', 'consent', 'even', 'reluctant', 'variety', 'simply', 'may', 'sit', 'stunned', 'silence', 'figure', 'way', 'sabotaging', 'plan', 'regain', 'composure', 'quick', 'reaction', 'slow', 'reaction', 'facility', 'rotate', 'people', 'discover', 'quick', 'reaction', 'facility', 'qrf', 'try', 'get', 'work', 'done', 'bogging', 'work', 'leaving', 'slow', 'reaction', 'facility', 'srf', 'nothing', 'thus', 'becoming', 'faster', 'two', 'complexity', 'attracts', 'brilliance', 'kiss', 'keep', 'simple', 'stupid', 'principle', 'fun', 'certainly', 'professional', 'approach', 'want', 'brilliant', 'people', 'work', 'make', 'complex', 'demanding', 'true', 'professional', 'spend', '20', 'hour', 'computer', 'writing', 'one', 'time', 'use', 'program', 'replace', '10', 'hour', 'clerical', 'work', 'anyway', '20', 'hour', 'professional', 'rate', 'pay', '10', 'hour', 'clerical', 'rate', 'also', 'intellectually', 'rewarding', 'greatest', 'achievement', 'use', 'one', 'finest', 'professional', 'talent', 'accomplish', 'something', 'need', 'done', 'bad', 'guy', 'replaced', 'ever', 'rejoice', 'departure', 'someone', 'get', 'along', 'find', 'replica', 'ha', 'shown', 'trying', 'make', 'turn', 'someone', 'tailgating', 'pulled', 'side', 'street', 'alley', 'find', 'two', 'car', 'right', 'behind']\n",
            "llong.hum \t ['introduction', 'notebook', 'lazarus', 'long', 'memorable', 'character', 'created', 'work', 'robert', 'heinlein', 'lazarus', 'long', 'immortal', 'stand', 'beyond', 'rest', 'lazarus', 'first', 'appeared', 'methuselah', 'child', '1941', 'central', 'character', 'novel', 'helped', 'establish', 'golden', 'age', 'science', 'fiction', 'astounding', 'magazine', 'establish', 'heinlein', 'future', 'history', 'series', 'pinnacle', 'achievement', 'science', 'fiction', 'present', 'heinlein', 'fan', 'wait', 'thirty', 'year', 'publication', 'time', 'enough', 'love', 'lazarus', 'return', 'center', 'stage', 'heinlein', 'future', 'history', 'massive', 'series', 'story', 'novel', 'collected', 'past', 'tomorrow', 'orphan', 'sky', 'wa', 'completed', 'appearance', 'time', 'enough', 'love', 'longest', 'highly', 'developed', 'heinlein', 'novel', 'lazarus', 'long', 'oldest', 'living', 'member', 'human', 'race', 'life', 'travel', 'time', 'space', 'unifies', 'great', 'series', 'lazarus', 'never', 'die', 'humanity', 'dream', 'immortality', 'embodied', 'wily', 'lovable', 'character', 'wise', 'experience', 'thousand', 'year', 'life', 'continued', 'zest', 'life', 'ironic', 'appreciation', 'success', 'failure', 'human', 'society', 'make', 'observation', 'notebook', 'originally', 'published', 'interlude', 'heinlein', 'huge', 'novel', 'eternal', 'life', 'essential', 'reading', 'notebook', 'lazarus', 'long', 'entertaining', 'smallest', 'detail', 'daily', 'fife', 'arching', 'abstraction', 'nature', 'human', 'condition', 'lazarus', 'comment', 'acute', 'lively', 'intelligent', 'notebook', 'lazarus', 'long', 'alias', 'woodrow', 'wilson', 'smith', 'mr', 'justice', 'lenox', 'corporal', 'ted', 'bronson', 'proscribed', 'prisoner', '83m2742', 'serenity', 'seraphin', 'et', 'al', 'ad', 'infinitum', 'oldest', 'living', 'member', 'human', 'race', 'virtue', 'unique', 'set', 'chromosome', 'clonal', 'rejuvenation', 'technique', 'finely', 'tuned', 'sense', 'rational', 'self', 'interest', 'ha', 'pioneered', 'eight', 'planet', 'survived', 'least', 'one', 'lynch', 'mob', 'many', 'wife', 'fought', 'fifteen', 'interstellar', 'war', 'made', 'lost', 'numerous', 'fortune', 'fathered', 'progeny', 'number', 'billion', 'read', 'sheer', 'enjoyment', 'ponder', 'didactic', 'message', 'copyright', '1973', 'robert', 'heinlein', 'always', 'store', 'beer', 'dark', 'place', 'data', 'date', 'one', 'animal', 'galaxy', 'dangerous', 'man', 'man', 'must', 'supply', 'indispensable', 'competition', 'ha', 'enemy', 'help', 'men', 'sentimental', 'woman', 'blur', 'thinking', 'certainly', 'game', 'rigged', 'let', 'stop', 'bet', 'win', 'priest', 'shaman', 'must', 'presumed', 'guilty', 'proved', 'innocent', 'always', 'listen', 'expert', 'tell', 'done', 'get', 'shot', 'fast', 'upset', 'long', 'enough', 'let', 'make', 'second', 'shot', 'perfect', 'expressed', 'figure', 'science', 'opinion', 'ha', 'long', 'known', 'one', 'horse', 'run', 'faster', 'another', 'one', 'difference', 'crucial', 'fake', 'fortuneteller', 'tolerated', 'authentic', 'soothsayer', 'shot', 'sight', 'cassandra', 'get', 'half', 'kicking', 'around', 'deserved', 'delusion', 'often', 'functional', 'mother', 'opinion', 'child', 'beauty', 'intelligence', 'goodness', 'et', 'cetera', 'ad', 'nauseam', 'keep', 'drowning', 'birth', 'scientist', 'bottle', 'washer', 'button', 'sorter', 'pacifist', 'male', 'contradiction', 'term', 'self', 'described', 'pacifist', 'pacific', 'simply', 'assume', 'false', 'color', 'wind', 'change', 'hoist', 'jolly', 'roger', 'nursing', 'doe', 'diminish', 'beauty', 'woman', 'breast', 'enhances', 'charm', 'making', 'look', 'lived', 'happy', 'generation', 'ignores', 'history', 'ha', 'past', 'future', 'poet', 'read', 'verse', 'public', 'may', 'nasty', 'habit', 'wonderful', 'world', 'ha', 'girl', 'small', 'change', 'often', 'found', 'seat', 'cushion', 'history', 'doe', 'record', 'anywhere', 'time', 'religion', 'ha', 'rational', 'basis', 'religion', 'crutch', 'people', 'strong', 'enough', 'stand', 'unknown', 'without', 'help', 'like', 'dandruff', 'people', 'religion', 'spend', 'time', 'money', 'seem', 'derive', 'considerable', 'pleasure', 'fiddling', 'amazing', 'much', 'mature', 'wisdom', 'resembles', 'tired', 'like', 'like', 'people', 'enemy', 'never', 'villain', 'eye', 'keep', 'mind', 'may', 'offer', 'way', 'make', 'friend', 'kill', 'without', 'hate', 'quickly', 'motion', 'adjourn', 'always', 'order', 'state', 'ha', 'inherent', 'right', 'survive', 'conscript', 'troop', 'long', 'run', 'state', 'ever', 'ha', 'roman', 'matron', 'used', 'say', 'son', 'come', 'back', 'shield', 'later', 'custom', 'declined', 'rome', 'strange', 'crime', 'human', 'legislated', 'nothing', 'blasphemy', 'amazing', 'obscenity', 'indecent', 'exposure', 'fighting', 'second', 'third', 'place', 'cheops', 'law', 'nothing', 'ever', 'get', 'built', 'schedule', 'within', 'budget', 'better', 'copulate', 'never', 'society', 'based', 'rule', 'protect', 'pregnant', 'woman', 'young', 'child', 'else', 'surplusage', 'excrescence', 'adornment', 'luxury', 'folly', 'must', 'dumped', 'emergency', 'preserve', 'prime', 'function', 'racial', 'survival', 'universal', 'morality', 'basic', 'possible', 'attempt', 'formulate', 'perfect', 'society', 'foundation', 'woman', 'child', 'first', 'witless', 'automatically', 'genocidal', 'nevertheless', 'starry', 'eyed', 'idealist', 'male', 'tried', 'endlessly', 'doubt', 'keep', 'trying', 'men', 'created', 'unequal', 'money', 'powerful', 'aphrodisiac', 'flower', 'work', 'almost', 'well', 'brute', 'kill', 'pleasure', 'fool', 'kill', 'hate', 'one', 'way', 'console', 'widow', 'remember', 'risk', 'need', 'arises', 'doe', 'must', 'able', 'shoot', 'dog', 'farm', 'make', 'nicer', 'make', 'worse', 'everything', 'excess', 'enjoy', 'flavor', 'life', 'take', 'big', 'bite', 'moderation', 'monk', 'may', 'better', 'live', 'jackal', 'dead', 'lion', 'better', 'still', 'live', 'lion', 'usually', 'easier', 'one', 'man', 'theology', 'another', 'man', 'belly', 'laugh', 'sex', 'friendly', 'otherwise', 'stick', 'mechanical', 'toy', 'sanitary', 'men', 'rarely', 'ever', 'manage', 'dream', 'god', 'superior', 'god', 'manner', 'moral', 'spoiled', 'child', 'never', 'appeal', 'man', 'better', 'nature', 'may', 'one', 'invoking', 'self', 'interest', 'give', 'leverage', 'little', 'girl', 'like', 'butterfly', 'need', 'excuse', 'peace', 'freedom', 'ever', 'count', 'avoid', 'making', 'irrevocable', 'decision', 'tired', 'hungry', 'circumstance', 'force', 'hand', 'think', 'ahead', 'place', 'clothes', 'weapon', 'find', 'dark', 'elephant', 'mouse', 'built', 'government', 'specification', 'committee', 'life', 'form', 'six', 'leg', 'brain', 'throughout', 'history', 'poverty', 'normal', 'condition', 'man', 'advance', 'permit', 'norm', 'exceeded', 'work', 'extremely', 'small', 'minority', 'frequently', 'despised', 'often', 'condemned', 'almost', 'always', 'opposed', 'right', 'thinking', 'people', 'whenever', 'tiny', 'minority', 'kept', 'creating', 'sometimes', 'happens', 'driven', 'society', 'people', 'slip', 'back', 'abject', 'poverty', 'known', 'bad', 'luck', 'mature', 'society', 'civil', 'servant', 'semantically', 'equal', 'civil', 'master', 'place', 'get', 'crowded', 'enough', 'require', 'id', 'social', 'collapse', 'far', 'away', 'time', 'go', 'elsewhere', 'best', 'thing', 'space', 'travel', 'made', 'possible', 'go', 'elsewhere', 'woman', 'property', 'husband', 'think', 'otherwise', 'living', 'dreamworld', 'second', 'best', 'thing', 'space', 'travel', 'distance', 'involved', 'make', 'war', 'difficult', 'usually', 'impractical', 'almost', 'always', 'unnecessary', 'probably', 'loss', 'people', 'since', 'war', 'race', 'popular', 'diversion', 'one', 'give', 'purpose', 'color', 'dull', 'stupid', 'life', 'great', 'boon', 'intelligent', 'man', 'fight', 'must', 'never', 'sport', 'zygote', 'gamete', 'way', 'producing', 'gamete', 'may', 'purpose', 'universe', 'hidden', 'contradiction', 'mind', 'people', 'love', 'nature', 'deploring', 'artificiality', 'man', 'ha', 'spoiled', 'nature', 'obvious', 'contradiction', 'lie', 'choice', 'word', 'imply', 'man', 'artifact', 'part', 'nature', 'beaver', 'dam', 'contradiction', 'go', 'deeper', 'prima', 'facie', 'absurdity', 'declaring', 'love', 'beaver', 'dam', 'erected', 'beaver', 'beaver', 'purpose', 'hatred', 'dam', 'erected', 'men', 'purpose', 'men', 'naturist', 'reveals', 'hatred', 'race', 'self', 'hatred', 'case', 'naturist', 'self', 'hatred', 'understandable', 'sorry', 'lot', 'hatred', 'strong', 'emotion', 'feel', 'toward', 'pity', 'contempt', 'rate', 'willy', 'nilly', 'man', 'beaver', 'sapiens', 'race', 'fortunately', 'like', 'part', 'race', 'made', 'men', 'woman', 'strike', 'fine', 'arrangement', 'perfectly', 'natural', 'believe', 'naturist', 'opposed', 'first', 'flight', 'old', 'earth', 'moon', 'unnatural', 'despoiling', 'nature', 'man', 'island', 'much', 'may', 'feel', 'act', 'individual', 'race', 'single', 'organism', 'always', 'growing', 'branching', 'must', 'pruned', 'regularly', 'healthy', 'necessity', 'need', 'argued', 'anyone', 'eye', 'see', 'organism', 'grows', 'without', 'limit', 'always', 'dy', 'poison', 'rational', 'question', 'whether', 'pruning', 'best', 'done', 'birth', 'incurable', 'sentimentalist', 'favor', 'former', 'method', 'killing', 'make', 'queasy', 'even', 'case', 'dead', 'alive', 'way', 'wanted', 'may', 'matter', 'taste', 'shaman', 'think', 'better', 'killed', 'war', 'die', 'childbirth', 'starve', 'misery', 'never', 'lived', 'may', 'right', 'like', 'democracy', 'based', 'assumption', 'million', 'men', 'wiser', 'one', 'man', 'missed', 'something', 'autocracy', 'based', 'assumption', 'one', 'man', 'wiser', 'million', 'men', 'let', 'play', 'decides', 'government', 'work', 'authority', 'responsibility', 'equal', 'coordinate', 'doe', 'insure', 'good', 'government', 'simply', 'insures', 'work', 'government', 'rare', 'people', 'want', 'run', 'thing', 'want', 'part', 'blame', 'used', 'called', 'backseat', 'driver', 'syndrome', 'fact', 'fact', 'shun', 'wishful', 'thinking', 'ignore', 'divine', 'revelation', 'forget', 'star', 'foretell', 'avoid', 'opinion', 'care', 'neighbor', 'think', 'never', 'mind', 'unguessable', 'verdict', 'history', 'fact', 'many', 'decimal', 'place', 'pilot', 'always', 'unknown', 'future', 'fact', 'single', 'clue', 'get', 'fact', 'stupidity', 'cured', 'money', 'education', 'legislation', 'stupidity', 'sin', 'victim', 'help', 'stupid', 'stupidity', 'universal', 'capital', 'crime', 'sentence', 'death', 'appeal', 'execution', 'carried', 'automatically', 'without', 'pity', 'god', 'omnipotent', 'omniscient', 'omnibenevolent', 'say', 'right', 'label', 'mind', 'capable', 'believing', 'three', 'divine', 'attribute', 'simultaneously', 'wonderful', 'bargain', 'check', 'please', 'cash', 'small', 'bill', 'courage', 'complement', 'fear', 'man', 'fearless', 'courageous', 'also', 'fool', 'two', 'highest', 'achievement', 'human', 'mind', 'twin', 'concept', 'loyalty', 'duty', 'whenever', 'twin', 'concept', 'fall', 'disrepute', 'get', 'fast', 'may', 'possibly', 'save', 'late', 'save', 'society', 'doomed', 'people', 'go', 'broke', 'big', 'way', 'never', 'miss', 'meal', 'poor', 'jerk', 'shy', 'half', 'slug', 'must', 'tighten', 'belt', 'truth', 'proposition', 'ha', 'nothing', 'credibility', 'vice', 'versa', 'anyone', 'cope', 'mathematics', 'fully', 'human', 'best', 'tolerable', 'subhuman', 'ha', 'learned', 'wear', 'shoe', 'bathe', 'make', 'mess', 'house', 'moving', 'part', 'rubbing', 'contact', 'require', 'lubrication', 'avoid', 'excessive', 'wear', 'honorific', 'formal', 'politeness', 'provide', 'lubrication', 'people', 'rub', 'together', 'often', 'young', 'untraveled', 'naive', 'unsophisticated', 'deplore', 'formality', 'empty', 'meaningless', 'dishonest', 'scorn', 'use', 'matter', 'pure', 'motive', 'thereby', 'throw', 'sand', 'machinery', 'doe', 'work', 'well', 'best', 'human', 'able', 'change', 'diaper', 'plan', 'invasion', 'butcher', 'hog', 'conn', 'ship', 'design', 'building', 'write', 'sonnet', 'balance', 'account', 'build', 'wall', 'set', 'bone', 'comfort', 'dying', 'take', 'order', 'give', 'order', 'cooperate', 'act', 'alone', 'solve', 'equation', 'analyze', 'new', 'problem', 'pitch', 'manure', 'program', 'computer', 'cook', 'tasty', 'meal', 'fight', 'efficiently', 'die', 'gallantly', 'specialization', 'insect', 'love', 'love', 'intensely', 'love', 'limit', 'many', 'love', 'person', 'time', 'enough', 'could', 'love', 'majority', 'decent', 'masturbation', 'cheap', 'clean', 'convenient', 'free', 'possibility', 'wrongdoing', 'go', 'home', 'cold', 'lonely', 'beware', 'altruism', 'based', 'self', 'deception', 'root', 'evil', 'tempted', 'something', 'feel', 'altruistic', 'examine', 'motif', 'root', 'self', 'deception', 'still', 'want', 'wallow', 'preposterous', 'notion', 'sapiens', 'ha', 'ever', 'dreamed', 'lord', 'god', 'creation', 'shaper', 'ruler', 'universe', 'want', 'saccharine', 'adoration', 'creature', 'swayed', 'payer', 'becomes', 'petulant', 'doe', 'receive', 'flattery', 'yet', 'absurd', 'fantasy', 'without', 'shred', 'evidence', 'bolster', 'pay', 'expense', 'oldest', 'largest', 'least', 'productive', 'industry', 'history', 'second', 'preposterous', 'notion', 'copulation', 'inherently', 'sinful', 'writing', 'necessarily', 'something', 'ashamed', 'private', 'wash', 'hand', 'afterwards', '100', 'placed', 'percent', 'interest', 'compounded', 'quarterly', '200', 'year', 'increase', '100', '000', '000', 'time', 'worth', 'nothing', 'dear', 'bore', 'trivia', 'burden', 'past', 'mistake', 'happiest', 'way', 'deal', 'man', 'never', 'tell', 'anything', 'doe', 'need', 'know', 'darling', 'true', 'lady', 'take', 'dignity', 'clothes', 'doe', 'whorish', 'best', 'time', 'modest', 'dignified', 'persona', 'requires', 'everybody', 'lie', 'sex', 'men', 'automaton', 'behaviorist', 'claim', 'behaviorist', 'psychologist', 'could', 'invented', 'amazing', 'nonsense', 'called', 'behaviorist', 'psychology', 'wrong', 'scratch', 'clever', 'wrong', 'phlogiston', 'chemist', 'shaman', 'forever', 'yacking', 'abut', 'snake', 'oil', 'miracle', 'prefer', 'real', 'mccoy', 'pregnant', 'woman', 'universe', 'ha', 'purpose', 'important', 'topping', 'woman', 'love', 'making', 'baby', 'hearty', 'help', 'never', 'heard', '10', 'thou', 'shalt', 'remember', 'eleventh', 'commandment', 'keep', 'wholly', 'touchstone', 'determine', 'actual', 'worth', 'intellectual', 'find', 'feel', 'astrology', 'tax', 'levied', 'benefit', 'taxed', 'thing', 'social', 'gambling', 'either', 'cut', 'bloke', 'heart', 'eat', 'sucker', 'like', 'choice', 'gamble', 'ship', 'lift', 'bill', 'paid', 'regret', 'first', 'time', 'wa', 'drill', 'instructor', 'wa', 'inexperienced', 'job', 'thing', 'taught', 'lad', 'must', 'got', 'killed', 'war', 'serious', 'matter', 'taught', 'inexperienced', 'competent', 'self', 'confident', 'person', 'incapable', 'jealousy', 'anything', 'jealousy', 'invariably', 'symptom', 'neurotic', 'insecurity', 'money', 'sincerest', 'flattery', 'woman', 'love', 'flattered', 'men', 'live', 'learn', 'live', 'long', 'whenever', 'woman', 'insisted', 'absolute', 'equality', 'men', 'invariably', 'wound', 'dirty', 'end', 'stick', 'make', 'superior', 'men', 'proper', 'tactic', 'demand', 'special', 'privilege', 'traffic', 'bear', 'never', 'settle', 'merely', 'equality', 'woman', 'equality', 'disaster', 'peace', 'extension', 'war', 'political', 'mean', 'plenty', 'elbowroom', 'pleasanter', 'much', 'safer', '11', 'one', 'man', 'magic', 'another', 'man', 'engineering', 'supernatural', 'null', 'word', 'phrase', 'simply', 'must', 'designates', 'something', 'need', 'done', 'go', 'without', 'saying', 'red', 'warning', 'course', 'mean', 'best', 'check', 'small', 'change', 'cliche', 'others', 'like', 'read', 'correctly', 'reliable', 'channel', 'marker', 'handicap', 'child', 'making', 'life', 'easy', 'rub', 'foot', 'happen', 'one', 'fretful', 'minority', 'creative', 'work', 'never', 'force', 'idea', 'abort', 'patient', 'give', 'birth', 'time', 'ripe', 'learn', 'wait', 'never', 'crowd', 'youngster', 'private', 'affair', 'sex', 'especially', 'growing', 'nerve', 'end', 'resent', 'quite', 'properly', 'invasion', 'privacy', 'oh', 'sure', 'make', 'mistake', 'business', 'made', 'mistake', 'never', 'understate', 'power', 'human', 'stupidity', 'always', 'tell', 'beautiful', 'especially', 'part', 'society', 'vote', 'may', 'candidate', 'measure', 'want', 'vote', 'certain', 'one', 'want', 'vote', 'case', 'doubt', 'vote', 'rule', 'rarely', 'go', 'wrong', 'blind', 'taste', 'consult', 'well', 'meaning', 'fool', 'always', 'one', 'around', 'ask', 'advice', 'vote', 'way', 'enables', 'good', 'citizen', 'wish', 'without', 'spending', 'enormous', 'amount', 'time', 'intelligent', 'exercise', 'franchise', 'requires', '12', 'sovereign', 'ingredient', 'happy', 'marriage', 'pay', 'cash', 'without', 'interest', 'charge', 'eat', 'household', 'budget', 'awareness', 'debt', 'eats', 'domestic', 'felicity', 'refuse', 'support', 'defend', 'state', 'claim', 'protection', 'state', 'killing', 'anarchist', 'pacifist', 'defined', 'murder', 'legalistic', 'sense', 'offense', 'state', 'using', 'deadly', 'weapon', 'inside', 'city', 'limit', 'creating', 'traffic', 'hazard', 'endangering', 'bystander', 'misdemeanor', 'however', 'state', 'may', 'reasonably', 'place', 'closed', 'season', 'exotic', 'asocial', 'animal', 'whenever', 'danger', 'becoming', 'extinct', 'authentic', 'buck', 'pacifist', 'ha', 'rarely', 'seen', 'earth', 'doubtful', 'survived', 'trouble', 'regrettable', 'biggest', 'mouth', 'smallest', 'brain', 'primate', 'small', 'mouthed', 'variety', 'anarchist', 'ha', 'spread', 'galaxy', 'wave', 'front', 'diaspora', 'need', 'protect', 'often', 'shoot', 'back', 'another', 'ingredient', 'happy', 'marriage', 'budget', 'luxury', 'first', 'still', 'another', 'see', 'ha', 'desk', 'keep', 'hand', 'another', 'family', 'argument', 'turn', 'right', 'apologize', 'god', 'split', 'myriad', 'part', 'might', 'friend', 'may', 'true', 'sound', 'good', 'sillier', 'theology', 'stay', 'young', 'requires', 'unceasing', 'cultivation', 'ability', 'unlearn', 'old', 'falsehood', 'doe', 'history', 'record', 'case', 'majority', 'wa', 'right', 'fox', 'gnaws', 'smile', '13', 'critic', 'man', 'creates', 'nothing', 'thereby', 'feel', 'qualified', 'judge', 'work', 'creative', 'men', 'logic', 'unbiased', 'hate', 'creative', 'people', 'equally', 'money', 'truthful', 'man', 'speaks', 'honor', 'make', 'pay', 'cash', 'never', 'frighten', 'little', 'man', 'kill', 'sadistic', 'scoundrel', 'fool', 'tell', 'bald', 'truth', 'social', 'occasion', 'sad', 'little', 'lizard', 'told', 'wa', 'brontosaurus', 'mother', 'side', 'laugh', 'people', 'boast', 'ancestry', 'often', 'little', 'else', 'sustain', 'humoring', 'cost', 'nothing', 'add', 'happiness', 'world', 'happiness', 'always', 'short', 'supply', 'handling', 'stinging', 'insect', 'move', 'slowly', 'matter', 'fact', 'world', 'blunder', 'fantasy', 'dull', 'fantasy', 'real', 'world', 'strange', 'wonderful', 'difference', 'science', 'fuzzy', 'subject', 'science', 'requires', 'reasoning', 'subject', 'merely', 'require', 'scholarship', 'copulation', 'spiritual', 'essence', 'merely', 'friendly', 'exercise', 'second', 'thought', 'strike', 'merely', 'copulation', 'merely', 'even', 'happy', 'pastime', 'two', 'stranger', 'copulation', 'spiritual', 'best', 'much', 'physical', 'coupling', 'different', 'kind', 'well', 'degree', 'saddest', 'feature', 'homosexuality', 'wrong', 'sinful', 'even', 'lead', 'progeny', 'difficult', 'reach', 'spiritual', 'union', 'impossible', 'card', 'stacked', '14', 'touch', 'fundamental', 'sense', 'baby', 'experience', 'born', 'long', 'learns', 'use', 'sight', 'hearing', 'taste', 'human', 'ever', 'cease', 'need', 'keep', 'child', 'short', 'pocket', 'money', 'long', 'hug', 'secrecy', 'beginning', 'tyranny', 'greatest', 'productive', 'force', 'human', 'selfishness', 'wary', 'strong', 'drink', 'make', 'shoot', 'tax', 'collector', 'miss', 'profession', 'shaman', 'ha', 'many', 'advantage', 'offer', 'high', 'status', 'safe', 'livelihood', 'free', 'work', 'dreary', 'sweaty', 'sense', 'society', 'offer', 'legal', 'privilege', 'immunity', 'granted', 'men', 'hard', 'see', 'man', 'ha', 'given', 'mandate', 'high', 'spread', 'tidings', 'joy', 'mankind', 'seriously', 'interested', 'taking', 'collection', 'pay', 'salary', 'cause', 'one', 'suspect', 'shaman', 'moral', 'level', 'con', 'man', 'lovely', 'work', 'stomach', 'whore', 'judged', 'criterion', 'professional', 'offering', 'service', 'pay', 'dentist', 'lawyer', 'hairdresser', 'physician', 'plumber', 'etc', 'professionally', 'competent', 'doe', 'give', 'good', 'measure', 'honest', 'client', 'possible', 'percentage', 'honest', 'competent', 'whore', 'higher', 'plumber', 'much', 'higher', 'lawyer', 'enormously', 'higher', 'professor', 'minimize', 'therbligs', 'becomes', 'automatic', 'double', 'effective', 'lifetime', 'thereby', 'give', 'time', 'enjoy', 'butterfly', 'kitten', 'rainbow', 'noticed', 'much', 'look', 'like', 'orchid', 'lovely', 'expertise', 'one', 'field', 'doe', 'carry', 'field', 'expert', 'often', 'think', 'narrower', 'field', 'knowledge', 'likely', 'think', '15', 'never', 'try', 'outstubborn', 'cat', 'tilting', 'windmill', 'hurt', 'windmill', 'yield', 'temptation', 'may', 'pas', 'way', 'waking', 'person', 'unnecessarily', 'considered', 'capital', 'crime', 'first', 'offense', 'go', 'hell', 'insult', 'direct', 'answer', 'snoopy', 'question', 'rate', 'correct', 'way', 'punctuate', 'sentence', 'start', 'course', 'none', 'business', 'place', 'period', 'word', 'use', 'excessive', 'force', 'supplying', 'moron', 'period', 'cutting', 'throat', 'momentary', 'pleasure', 'bound', 'get', 'talked', 'man', 'doe', 'insist', 'physical', 'beauty', 'woman', 'build', 'morale', 'realizes', 'beautiful', 'noticed', 'first', 'skunk', 'better', 'company', 'person', 'pride', 'frank', 'fair', 'love', 'war', 'contemptible', 'lie', 'beware', 'black', 'swan', 'fallacy', 'deductive', 'logic', 'tautological', 'way', 'get', 'new', 'truth', 'manipulates', 'false', 'statement', 'readily', 'true', 'one', 'fail', 'remember', 'trip', 'perfect', 'logic', 'desig', 'soon', 'enough', 'know', 'fret', 'came', 'saw', 'conquered', 'original', 'latin', 'seems', 'garbled', 'try', 'last', 'word', 'might', 'get', 'lazarus', 'long', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing postingList and postingList dictionary for each word\n",
        "for singleWord in uniqueWords:\n",
        "    postingsList[singleWord] = [] \n",
        "    postingsListDictionary[singleWord] = []\n",
        "print(color.BOLD+color.BLUE+\"Number of unique words in whole dataset files:\"+color.END,len(postingsList))"
      ],
      "metadata": {
        "id": "Jgx2k7DbYTEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64bc082-6a80-4f59-b1ae-ea1dca765528"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94mNumber of unique words in whole dataset files:\u001b[0m 64405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filling values in corresponding postingList and postingList dictionary for each word \n",
        "l=[]\n",
        "for dataFileNumber in range(len(datasetFiles)):\n",
        "  tokens = datasetCorpus[datasetFiles[dataFileNumber]]\n",
        "  for singleWord in tokens:\n",
        "    if(datasetFiles[dataFileNumber] not in postingsList[singleWord]):\n",
        "      postingsList[singleWord].append(datasetFiles[dataFileNumber]) \n",
        "      postingsListDictionary[singleWord].append(dataFileNumber)"
      ],
      "metadata": {
        "id": "AJzY-tC5YToP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(color.BOLD+\"Displaying some posting list instances\"+color.END)\n",
        "print(\"Posting list of word 'lion': \",postingsList['lion'])\n",
        "print(\"Posting list of word 'stood': \",postingsList['stood'])\n",
        "print(\"Posting list of word 'great': \",postingsList['great'])\n",
        "print(\"Posting list of word 'moment': \",postingsList['good'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVYv1c3FtRRt",
        "outputId": "febb03bb-fcf9-41e6-e4b1-c30723109258"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDisplaying some posting list instances\u001b[0m\n",
            "Posting list of word 'lion':  ['murphys.txt', 'llong.hum', 'onetotwo.hum', 'deep.txt', 'drunk.txt', 'hecomes.jok', 'wagon.hum', 'lion.jok', 'puzzles.jok', 'lions.cat', 'tnd.1', 'boneles2.txt', 'dthought.txt', 'tpquotes.txt', 'japantv.txt', 'mindvox', 'lion.txt', 'booze1.fun', 'stuf10.txt', 'gd_gal.txt', 'christop.int', 'filmgoof.txt', 'dromes.txt', 'three.txt', 'bitnet.txt', 'collected_quotes.txt', 'computer.txt', 'murphy_l.txt']\n",
            "Posting list of word 'stood':  ['conan.txt', 'ivan.hum', 'lifeimag.hum', 'luggage.hum', 'lozerzon.hum', 'm0dzmen.hum', 'maecenas.hum', 'reasons.txt', 'montpyth.hum', 'news.hum', 'onetoone.hum', 'passage.hum', 'peatchp.hum', 'pizzawho.hum', 'phorse.hum', 'quest.hum', 'fuckyou2.txt', 'smurfkil.hum', 'soleleer.hum', 'stone.hum', 'lawyer.jok', 'marriage.hum', 'misc.1', 'xibovac.txt', 'childhoo.jok', 'nigel.5', 'pepsideg.txt', 'quux_p.oem', 'eskimo.nel', 'prac1.jok', 'prac3.jok', 'jc-elvis.inf', 'epitaph', 'quack26.txt', 'cogdis.txt', 'boneles2.txt', 'kanalx.txt', 'barney.txt', 'bmdn01.txt', 'flux_fix.txt', 'golnar.txt', 'iremember', 'nameisreo.txt', 'epi_tton.txt', 'epi_.txt', 'scratchy.txt', 'pracjoke.txt', 'cybrtrsh.txt', 'humor9.txt', 'cookie.1', 'minn.txt', 'practica.txt', 'homebrew.txt', 'insult.lst', 'insults1.txt', 'calculus.txt', 'indgrdn.txt', 'namaste.txt', 'lifeonledge.txt', 'mel.txt', 'grail.txt', 'mlverb.hum', 'ghostfun.hum', 'solders.hum', 'throwawa.hum', 'cabbage.txt', 'nihgel_8.9', 'cuchy.hum', 'engineer.hum', 'moose.txt']\n",
            "Posting list of word 'great':  ['mowers.txt', 'herb!.hum', 'conan.txt', 'imprrisk.hum', 'insanity.hum', 'ivan.hum', 'dym', 'test.jok', 'kilsmur.hum', 'killself.hum', 'kid_diet.txt', 'blooprs1.asc', 'lbinter.hum', 'murphys.txt', 'legal.hum', 'ludeinfo.txt', 'looser.hum', 'lifeimag.hum', 'lif&love.hum', 'llong.hum', 'losers86.hum', 'lozerzon.hum', 'ludeinfo.hum', 'losers84.hum', 'lozeuser.hum', 'luzerzo2.hum', 'm0dzmen.hum', 'maecenas.hum', 'madscrib.hum', 'meinkamp.hum', 'melodram.hum', 'mash.hum', 'mrscienc.hum', 'montpyth.hum', 'mydaywss.hum', 'calvin.txt', 'myheart.hum', 'news.hum', 'novel.hum', 'o-ttalk.hum', 'oldeng.hum', 'onetoone.hum', 'onetotwo.hum', 'passage.hum', 'peatchp.hum', 'pizzawho.hum', 'phorse.hum', 'poll2res.hum', 'popmusi.hum', 'popconc.hum', 'dieter.txt', 'prawblim.hum', 'quest.hum', 'radiolaf.hum', 'ratings.hum', 'research.hum', 'docspeak.txt', 'rockmus.hum', 'rocking.hum', 'ripoffpc.hum', 'spydust.hum', 'soleleer.hum', 'stone.hum', 'sungenu.hum', 'top10.txt', \"terrmcd'.hum\", 'texican.dic', 'televisi.hum', 'lawyer.jok', 'let.go', 'koans.txt', 'trekwes.hum', 'thermite.ana', 'truthlsd.hum', 'voltron.hum', 'twinkie.txt', 'wetdream.hum', 'yjohncse.hum', 'zodiac.hum', 'worldend.hum', 'limerick.jok', 'urban.txt', 'lines.jok', 'tshirts.jok', 'wagit.txt', 'st_silic.txt', 'strine.txt', 'marriage.hum', 'misc.1', 'woolly_m.amm', 'lozers', 'xibovac.txt', 'climbing.let', 'childhoo.jok', 'co-car.jok', 'nigel.3', 'fartinfo.txt', 'nigel.4', 'gd_guide.txt', 'gas.txt', 'pournell.spo', 'pukeprom.jok', 'quux_p.oem', 'pickup.txt', 'eskimo.nel', 'prac1.jok', 'snipe.txt', 'prac3.jok', 'televisi.txt', 'idaho.txt', 'inlaws1.txt', 'kloo.txt', 'woodbine.txt', 'jargon.phd', 'letter.txt', 'letgosh.txt', 'rednecks.txt', 'prac4.jok', 'odd_to.obs', 'luvstory.txt', 'smurfs.cc', 'oracle.jok', 'oxymoron.jok', 'studentb.txt', 'telecom.q', 'shrink.news', 'number.killer', 'quick.jok', 'resrch_phrase', 'quotes.bug', 'shorties.jok', 'progrs.gph', 'quotes.jok', 'top10.elf', 'texican.lex', 'tnd.1', 'squids.gph', 'bw-phwan.hat', 'wisdom', 'firstaid.inf', 'ambrose.bie', 'oldtime.sng', 'anim_lif.txt', 'bad', 'talkbizr.txt', 'variety1.asc', 'variety3.asc', 'engrhyme.txt', 'epitaph', 'variety2.asc', 'various.txt', 'quack26.txt', 'godmonth.txt', 'cogdis.txt', 'b12.txt', 'newcoke.txt', 'chili.txt', 'mead.rcp', 'jerky.rcp', 'vegan.rcp', 'brewing', 'candy.txt', 'drinks.gui', 'pepper.txt', 'x-drinks.txt', 'fiber.txt', 'oculis.rcp', 'appetiz.rcp', 'fajitas.rcp', 'boneles2.txt', 'tpquotes.txt', 'gingbeer.txt', 'women.jok', 'wrdnws3.txt', 'ads.txt', 'gd_flybd.txt', 'earp', 'epikarat.txt', 'widows', 'twinpeak.txt', 'barney.txt', 'bmdn01.txt', 'libraway.txt', 'wisconsi.txt', 'sanshop.txt', 'top10st1.txt', 'top10st2.txt', 'transp.txt', 'bnb_quot.txt', 'bozo_tv.leg', 'coffee.txt', 'jason.fun', 'oliver.txt', 'oliver02.txt', 'sf-zine.pub', 'subrdead.hum', 'classicm.hum', 'fearcola.hum', 'horflick.txt', 'thievco.txt', 'aeonint.txt', 'mindvox', 'lp-assoc.txt', 'flux_fix.txt', 'packard.txt', 'moslem.txt', 'mov_rail.txt', 'missdish', 'consp.txt', 'food', 'foodtips', 'docdict.txt', 'drive.txt', 'cgs_lst.txt', 'golnar.txt', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'stuf10.txt', 'stuf11.txt', 'mog-history', 'dalive', 'kilroy', 'msorrow', 'nameisreo.txt', 'episimp2.txt', 'epi_tton.txt', 'epi_.txt', 'epi_merm.txt', 'epi_rns.txt', 'outlimit.txt', 'lost.txt', 'scratchy.txt', 'epiquest.txt', 'gd_ol.txt', 'gd_hhead.txt', 'gd_ql.txt', 'adrian_e.faq', 'allusion', 'anime.cli', 'anime.lif', 'wacky.ani', 'cast.lis', 'christop.int', 'chung.iv', 'clancy.txt', 'comic_st.gui', 'cultmov.faq', 'nukewar.txt', 'prac2.jok', 'cybrtrsh.txt', 'how2dotv.txt', 'filmgoof.txt', 'films_gl.txt', 'fegg!int.txt', 'feggaqui.txt', 'feggmagi.txt', 'normquot.txt', 'history2.oop', 'humor9.txt', 'cookie.1', 'coffee.faq', 'english.txt', 'figure_1.txt', 'crzycred.lst', 'arnold.txt', 'avengers.lis', 'blackadd', 'doc-says.txt', 'free-cof.fee', 'tarot.txt', 'c0dez.txt', 'beer.txt', 'freudonseuss.txt', 'booze.fun', 'diesmurf.txt', 'get.drunk.cheap', 'practica.txt', 'dead2.txt', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'radexposed.txt', 'pat.txt', 'hack7.txt', 'happyhack.txt', 'skincat', 'homebrew.txt', 'hop.faq', 'necropls.txt', 'focaccia.brd', 'goldwatr.txt', 'insult.lst', 'insults1.txt', 'orgfrost.bev', 'hackmorality.txt', 'berryeto.bev', 'jalapast.dip', 'montoys.txt', 'stagline.txt', 'bredcake.des', 'advrtize.txt', 'bitnet.txt', 'all_grai', 'beer.gam', 'beesherb.txt', 'bw.txt', 'acetab1.txt', 'brush1.txt', 'proudlyserve.txt', 'bnbeg2.4.txt', 'byfb.txt', 'collected_quotes.txt', 'childrenbooks.txt', 'llamas.txt', 'horoscope.txt', 'fireplacein.txt', 'enlightenment.txt', 'turing.shr', 'computer.txt', 'indgrdn.txt', 'sawyer.txt', 'aids.txt', 'reeves.txt', 'chickenheadbbs.txt', 'phxbbs-m.txt', 'ukunderg.txt', 'lifeonledge.txt', 'lipkovits.txt', 'basehead.txt', 'draxamus.txt', 'apsnet.txt', 'lansing.txt', 'mel.txt', 'grail.txt', 'hackingcracking.txt', 'adameve.hum', 'charity.hum', 'mlverb.hum', 'poets.hum', 'reconcil.hum', 'drugshum.hum', 'throwawa.hum', 'zen.txt', 'adcopy.hum', 'b-2.jok', 'suicide2.txt', 'alflog.txt', 'atombomb.hum', 'badday.hum', 'catin.hat', 'argotdic.txt', 'browneco.hum', 'boe.hum', 'btcisfre.hum', 'cabbage.txt', 'butwrong.hum', 'merry.txt', 'catballs.hum', 'nihgel_8.9', 'catranch.hum', 'cbmatic.hum', 'number_k.ill', 'manners.txt', 'rabbit.txt', 'dark.suc', 'acronyms.txt', 'defectiv.hum', 'art-fart.hum', 'murphy_l.txt', 'enquire.hum', 'middle.age', 'moose.txt', 'relative.ada', 'freshman.hum', 'resrch_p.hra', 'female.jok', 'goforth.hum']\n",
            "Posting list of word 'moment':  ['mowers.txt', 'herb!.hum', 'hate.hum', 'icm.hum', 'howlong.hum', 'coyote.txt', 'horoscop.jok', 'insanity.hum', 'murphy.txt', 'impurmat.hum', 'interv.hum', 'bless.bc', 'ivan.hum', 'jac&tuu.hum', 'terms.hum', 'dym', 'test.jok', 'kilsmur.hum', 'killself.hum', 'lawsuniv.hum', 'testchri.txt', 'test2.jok', 'lbinter.hum', 'motrbike.jok', 'murphys.txt', 'legal.hum', 'looser.hum', 'lif&love.hum', 'office.txt', 'llong.hum', 'luggage.hum', 'lozerzon.hum', 'losers84.hum', 'lozeuser.hum', 'luzerzo2.hum', 'm0dzmen.hum', 'madscrib.hum', 'meinkamp.hum', 'pun.txt', 'mtm.hum', 'melodram.hum', 'mash.hum', 'miamadvi.hum', 'f_tang.txt', 'montpyth.hum', 'mutate.hum', 'mydaywss.hum', 'calvin.txt', 'myheart.hum', 'naivewiz.hum', 'news.hum', 'nukeplay.hum', 'nuke.hum', 'novel.hum', 'oilgluts.hum', 'o-ttalk.hum', 'oldeng.hum', 'chunnel.txt', 'onetoone.hum', 'p-law.hum', 'onetotwo.hum', 'ozarks.hum', 'deep.txt', 'opinion.hum', 'passage.hum', 'parabl.hum', 'planeget.hum', 'peatchp.hum', 'phony.hum', 'pizzawho.hum', 'phorse.hum', 'devils.jok', 'popconc.hum', 'dieter.txt', 'psilaine.hum', 'dead-r', 'quest.hum', 'rapmastr.hum', 'reagan.hum', 'fuckyou2.txt', 'research.hum', 'reddye.hum', 'docspeak.txt', 'rockmus.hum', 'rocking.hum', 't_zone.jok', 'corporat.txt', 'smackjok.hum', 'jokes', 'skippy.hum', 'takenote.jok', 'smurfkil.hum', 'social.hum', 'spydust.hum', 'soleleer.hum', 'terbear.txt', 'socecon.hum', 'spider.hum', 'stone.hum', 'top10.txt', \"terrmcd'.hum\", 'texican.dic', 'televisi.hum', 'test.hum', 'textgrap.hum', 'thecube.hum', 'tickmoon.hum', 'lawyer.jok', 'toxcwast.hum', 'truths.hum', 'trekwes.hum', 'tribble.hum', 'thermite.ana', 'voltron.hum', 'whoon1st.hum', 'whoops.hum', 'twinkie.txt', 'wetdream.hum', 'yjohncse.hum', 'zodiac.hum', 'worldend.hum', 'xtermin8.hum', 'yuppies.hum', 'trukdeth.txt', 'hotnnot.hum', 'urban.txt', 'lines.jok', 'tshirts.jok', 'waitress.txt', 'vaguemag.90s', 'wagit.txt', 'lotsa.jok', 'strine.txt', 'lion.jok', 'welfare.txt', 'misc.1', 'math.1', 'wimptest.txt', 'woodbugs.txt', 'math.2', 'woolly_m.amm', 'chainltr.txt', 'xibovac.txt', 'climbing.let', 'church.sto', 'commutin.jok', 'nigel.10', 'elephant.fun', 'nigel.3', 'fartinfo.txt', 'fascist.txt', 'jokes.txt', 'nigel.5', 'nigel.6', 'fuck!.txt', 'lawskool.txt', 'nigel.7', 'good.txt', 'gd_guide.txt', 'grammar.jok', 'gas.txt', 'polemom.txt', 'pepsideg.txt', 'polly.txt', 'pournell.spo', 'polly_.new', 'moonshin', 'psych_pr.quo', 'pukeprom.jok', 'puzzles.jok', 'quux_p.oem', 'leech.txt', 'tuflife.txt', 'pickup.txt', 'eskimo.nel', 'prac1.jok', 'stereo.txt', 'prac3.jok', 'how_to_i.pro', 'horoscop.txt', 'htswfren.txt', 'smokers.txt', 'televisi.txt', 'sysadmin.txt', 'idaho.txt', 'inlaws1.txt', 'insuranc.sty', 'jc-elvis.inf', 'realest.txt', 'kloo.txt', 'psycho.txt', 'jargon.phd', 'sysman.txt', 'letter.txt', 'letter_f.sch', 'rednecks.txt', 'lampoon.jok', 'prac4.jok', 'lucky.cha', 'luvstory.txt', 'smurfs.cc', 'oracle.jok', 'oxymoron.jok', 'princess.brd', 'mr.rogers', 'telecom.q', 'nosuch_nasfic', 'popmach', 'quick.jok', 'shorties.jok', 'quotes.jok', 'texican.lex', 'welfare', 'tnd.1', 'quotes.txt', 'squids.gph', 'bw-phwan.hat', 'aussie.lng', 'wisdom', 'firstaid.inf', 'outawork.erl', 'oldtime.sng', 'oasis', 'oxymoron.txt', 'anim_lif.txt', 'bad', 'wood', 'talkbizr.txt', 'variety1.asc', 'variety3.asc', 'empeval.txt', 'english', 'epitaph', 'variety2.asc', 'q.pun', 'quack26.txt', 'godmonth.txt', 'gown.txt', 'cogdis.txt', 'b12.txt', 'cooking.fun', 'newcoke.txt', 'candybar.fun', 'snapple.rum', 'chili.txt', 'mead.rcp', 'beginn.ers', 'meat2.txt', 'jerky.rcp', 'vegan.rcp', 'brewing', 'curry.hrb', 'candy.txt', 'drinks.gui', 'pepper.txt', 'x-drinks.txt', 'oculis.rcp', 'appetiz.rcp', 'fajitas.rcp', 'kashrut.txt', 'bread.txt', 'hotel.txt', 'boneles2.txt', 'dthought.txt', 'tpquote2.txt', 'tpquotes.txt', 'ads.txt', 'adt_miam.txt', 'earp', 'epikarat.txt', 'kanalx.txt', 'twinpeak.txt', 'lazarus.txt', 'barney.txt', 'libraway.txt', 'wisconsi.txt', 'sanshop.txt', 'top10st1.txt', 'top10st2.txt', 'transp.txt', 'mundane.v2', 'japantv.txt', 'bnb_quot.txt', 'bored.txt', 'sw_err.txt', 'skippy.txt', 'smurf-03.txt', 'smurf_co.txt', 'coffee.txt', 'jason.fun', 'econridl.fun', 'oliver.txt', 'sf-zine.pub', 'subrdead.hum', 'topten.hum', 'how2bgod.txt', 'netmask.txt', 'chickens.txt', 'classicm.hum', 'cmu.share', 'fearcola.hum', 'horflick.txt', 'alcohol.hum', 'beer-g', 'beergame.hum', 'dining.out', 'thievco.txt', 'aeonint.txt', 'mindvox', 'deathhem.txt', 'lp-assoc.txt', 'flux_fix.txt', 'packard.txt', 'modstup', 'missdish', 'consp.txt', 'food', 'cake.rec', 'docdict.txt', 'drinks.txt', 'bhang.fun', 'golnar.txt', 'rns_bwl.txt', 'rns_ency.txt', 'stuf10.txt', 'stuf11.txt', 'fusion.gal', 'sfmovie.txt', 'mog-history', 'kilroy', 'msorrow', 'nameisreo.txt', 'inquirer.txt', 'bnbguide.txt', 'epi_tton.txt', 'epi_bnb.txt', 'epi_.txt', 'epi_merm.txt', 'epi_rns.txt', 'outlimit.txt', 'gd_gal.txt', 'lost.txt', 'scratchy.txt', 'gd_frasr.txt', 'gd_ol.txt', 'gd_hhead.txt', 'gd_tznew.txt', 'gd_ql.txt', 'a_tv_t-p.com', 'allfam.epi', 'allusion', 'anime.cli', 'anime.lif', 'wacky.ani', 'wkrp.epi', 'christop.int', 'chung.iv', 'clancy.txt', 'comic_st.gui', 'cultmov.faq', 'facedeth.txt', 'nukewar.txt', 'number', 'prac2.jok', 'pracjoke.txt', 'cybrtrsh.txt', 'hstlrtxt.txt', 'homermmm.txt', 'filmgoof.txt', 'fegg!int.txt', 'feggaqui.txt', 'feggmagi.txt', 'normquot.txt', 'ppbeer.txt', 'headlnrs', 'humor9.txt', 'cookie.1', 'cooplaws', 'coffee.faq', 'modemwld.txt', 'minn.txt', 'oam-001.txt', 'college.txt', 'figure_1.txt', 'crzycred.lst', 'bbc_vide.cat', 'beauty.tm', 'blackadd', 'doc-says.txt', 'free-cof.fee', 'resolutn.txt', 'swearfrn.hum', 'tarot.txt', 'c0dez.txt', 'beer.txt', 'butcher.txt', 'curiousgeorgie.txt', 'annoy.fascist', 'booze.fun', 'diesmurf.txt', 'get.drunk.cheap', 'practica.txt', 'dead2.txt', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'radexposed.txt', 'pat.txt', 'hack7.txt', 'happyhack.txt', 'airlines', 'skincat', 'homebrew.txt', 'hop.faq', 'necropls.txt', 'three.txt', 'feista01.dip', 'garlpast.vgn', 'texbeef.txt', 'wonton.txt', 'yogurt.asc', 'insult.lst', 'insults1.txt', 'penndtch', 'jon.txt', 'hackmorality.txt', 'baklava.des', 'banana01.brd', 'jawgumbo.fis', 'shuimai.txt', 'bredcake.des', 'blkbnsrc.vgn', 'bread.rcp', 'butstcod.fis', 'caramels.des', 'advrtize.txt', 'bitnet.txt', 'oldtime.txt', 'oakwood.txt', 'all_grai', 'amchap2.txt', 'beer.gam', 'beergame.txt', 'beer-gui', 'jimhood.txt', 'critic.txt', 'booknuti.txt', 'aniherb.txt', 'beesherb.txt', 'antibiot.txt', 'curry.txt', 'arthriti.txt', 'bw.txt', '1st_aid.txt', 'brush1.txt', 'bnbeg2.4.txt', 'calculus.txt', 'byfb.txt', 'paddingurpapers.txt', 'collected_quotes.txt', 'coffeebeerwomen.txt', 'girlspeak.txt', 'confucius_say.txt', 'fireplacein.txt', 'labels.txt', 'enlightenment.txt', 'trekfume.txt', 'valujet.txt', 'computer.txt', 'jokes1.txt', 'cops.txt', 'bhb.ill', 'bond-2.txt', 'indgrdn.txt', 'insect1.txt', 'quantity.001', 'sawyer.txt', 'exidy.txt', 'deadlysins.txt', 'planetzero.txt', 'aids.txt', 'reeves.txt', 'chickenheadbbs.txt', 'phxbbs-m.txt', 'ukunderg.txt', 'crazy.txt', 'lifeonledge.txt', 'basehead.txt', 'exylic.txt', 'onan.txt', 'teevee.hum', 'mel.txt', 'grail.txt', 'hackingcracking.txt', 'bingbong.hum', 'mlverb.hum', 'poets.hum', 'modest.hum', 'throwawa.hum', 'shameonu.hum', 'symbol.hum', 'whatthe.hum', 'spacever.hum', 'hammock.hum', 'memory.hum', 'sigs.txt', 'nigel.1', 'abbott.txt', 'suicide2.txt', 'alcatax.txt', 'nigel10.txt', 'alflog.txt', 'cancer.rat', 'beer.hum', 'beapimp.hum', 'bigpic1.hum', 'argotdic.txt', 'blackhol.hum', 'browneco.hum', 'cold.fus', 'boe.hum', 'btcisfre.hum', 'butwrong.hum', 'merry.txt', 'catballs.hum', 'catranch.hum', 'making_y.wel', 'coldfake.hum', 'comrevi1.hum', 'manners.txt', 'readme.bat', 'failure.txt', 'cuchy.hum', 'acronyms.txt', 'defectiv.hum', 'roadpizz.txt', 'art-fart.hum', 'deterior.hum', 'murphy_l.txt', 'engineer.hum', 'enquire.hum', 'bad.jok', 'firstaid.txt', 'flattax.hum', 'moose.txt', 'freshman.hum', 'fwksfun.hum', 'men&wome.txt', 'gohome.hum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisHKY_8Q0-r"
      },
      "source": [
        "**Defining OR, NOT, ANDNOT, ORNOT operations functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pX7_XR6CpDOM"
      },
      "outputs": [],
      "source": [
        "#Creating function that will perform AND operation between the two postings lists\n",
        "def AND(postingsListOne, postingsListTwo): \n",
        "  computationCounter = 0\n",
        "  output8=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(postingsListOne) and j<len(postingsListTwo)): \n",
        "    postingsListOne[i] = int(postingsListOne[i])\n",
        "    postingsListTwo[j] = int(postingsListTwo[j])\n",
        "    if(postingsListOne[i] == postingsListTwo[j]):\n",
        "      computationCounter+=1\n",
        "      output8.append(postingsListOne[i]) \n",
        "      i+=1\n",
        "      j+=1\n",
        "    else:\n",
        "      if(postingsListOne[i]<postingsListTwo[j]): \n",
        "        computationCounter+=1\n",
        "        i+=1\n",
        "      else:\n",
        "        computationCounter+=1\n",
        "        j+=1\n",
        "  return output8, computationCounter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "N1ZODn-RpGjj"
      },
      "outputs": [],
      "source": [
        "#Creating function that will perform OR operation between the two postings lists\n",
        "def OR(postingsListOne, postingsListTwo): \n",
        "  computationCounter=0\n",
        "  output7=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(postingsListOne) and j<len(postingsListTwo)): \n",
        "    postingsListOne[i] = int(postingsListOne[i])\n",
        "    postingsListTwo[j] = int(postingsListTwo[j])\n",
        "    if(postingsListOne[i]==postingsListTwo[j]): \n",
        "      computationCounter+=1\n",
        "      output7.append(postingsListOne[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    else:\n",
        "      if(postingsListOne[i]<postingsListTwo[j]): \n",
        "        computationCounter+=1\n",
        "        output7.append(postingsListOne[i])\n",
        "        i+=1\n",
        "      else:\n",
        "        computationCounter+=1\n",
        "        output7.append(postingsListTwo[j])\n",
        "        j+=1\n",
        "  if(i==len(postingsListOne)): \n",
        "    output7 = output7+postingsListTwo[j:] \n",
        "    computationCounter+=1\n",
        "  else:\n",
        "    output7= output7+postingsListOne[i:] \n",
        "    computationCounter+=1\n",
        "  return output7,computationCounter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "O3pEo0DEpNta"
      },
      "outputs": [],
      "source": [
        "#Creating function that will perform NOT operation on the given postings list\n",
        "def NOT(postings):\n",
        "    computationCounter=0\n",
        "    indexList = []\n",
        "    output6 =[]\n",
        "    for iterator in range(len(datasetFiles)): \n",
        "        indexList.append(iterator)\n",
        "    currentMainIndex=0\n",
        "    currentPostingsIndex=0\n",
        "    for iterator in range(postings[len(postings)-1]):\n",
        "        if(indexList[currentMainIndex]==postings[currentPostingsIndex] ):\n",
        "            currentPostingsIndex +=1\n",
        "            currentMainIndex +=1\n",
        "        else:\n",
        "            output6.append(indexList[currentMainIndex])\n",
        "            currentMainIndex +=1\n",
        "    output6 = output6+indexList[currentMainIndex+1:]\n",
        "    return output6,0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-BqThG2hpRpz"
      },
      "outputs": [],
      "source": [
        "#Creating function that will perform ANDNOT operation between the two postings lists\n",
        "def ANDNOT(postingsListOne, postingsListTwo):\n",
        "    postingsListThree,num1 = NOT(postingsListTwo) \n",
        "    output5,num2 = AND(postingsListOne, postingsListThree) \n",
        "    totalComputations = num1+num2 \n",
        "    return output5,totalComputations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "my8dJ80GpUqb"
      },
      "outputs": [],
      "source": [
        "#Creating function that will perform ORNOT operation between the two postings lists\n",
        "def ORNOT(postingsListOne, postingsListTwo):\n",
        "    postingsListThree,num1 = NOT(postingsListTwo)\n",
        "    output4,num2 = OR(postingsListOne, postingsListThree)\n",
        "    totalComputations =num1+num2\n",
        "    return output4,totalComputations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ69adhwRF3T"
      },
      "source": [
        " **Preprocessing input query and Defining driver and helper function to start the exection and perform desired operation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Wc8MkadQpXlE"
      },
      "outputs": [],
      "source": [
        "#Creating function that will call given operation function on the given two postings list\n",
        "def helperFunction(task, postingsListOne, postingsListTwo):\n",
        "    if(task == 'AND'): \n",
        "        output3,totalComputations = AND(postingsListOne, postingsListTwo)\n",
        "    if(task == 'OR'): \n",
        "        output3,totalComputations = OR(postingsListOne, postingsListTwo) \n",
        "    if(task == 'ANDNOT'):\n",
        "        output3,totalComputations= ANDNOT(postingsListOne, postingsListTwo)\n",
        "    if(task == 'ORNOT'):\n",
        "        output3,totalComputations = ORNOT(postingsListOne, postingsListTwo)\n",
        "    return output3,totalComputations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function findResults() will preprocess the input query, generate tokens from that preprocessed query and \n",
        "take input operations from user and will print the desired output results\n",
        "'''\n",
        "def findResultsNew():\n",
        "    print(color.BOLD+'Enter Query'+color.END)\n",
        "    inputQuery = input()                                                                                                #Taking input query\n",
        "    print()\n",
        "\n",
        "    inputQuery = inputQuery.lower()                                                                                     #Conversion to lower\n",
        "\n",
        "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''                                                                          #Removing punctuations\n",
        "    for ele in inputQuery:               \n",
        "      if ele in punc:  \n",
        "        inputQuery = inputQuery.replace(ele, \" \")\n",
        "\n",
        "    inputQuery = re.sub(r'[\\!@#$%^&\\()\\_+={}\\:\\;<>\\?/\\|\\-/\"\\']',\"\",inputQuery)                                          #Removing Junk symbols\n",
        "\n",
        "    tokens = word_tokenize(inputQuery)                                                                                  #Tokenization\n",
        "\n",
        "    tokens = [preProcessLemmatizer.lemmatize(word) for word in tokens]                                                  #Lemmatization\n",
        "\n",
        "    tokensWithoutStopwords= [word for word in tokens if not word in stopwords.words() and len(word)>1]                  #Removing Stopwords\n",
        "\n",
        "    print(color.BOLD+color.BLUE+'Generated tokens from your query: '+color.DARKCYAN,tokensWithoutStopwords,color.END) \n",
        "    print(color.BOLD+'\\nEnter a list containing', len(tokensWithoutStopwords) - 1, 'operations:'+color.END)\n",
        "\n",
        "    operations = input()                                                                                                #Taking input operations\n",
        "    operations = operations.replace('[','')\n",
        "    operations = operations.replace(']','')\n",
        "    operations = operations.replace(' ','') \n",
        "    operations = operations.upper() \n",
        "    operations = operations.split(',')  \n",
        "\n",
        "    if(len(operations) == (len(tokensWithoutStopwords)-1) ):\n",
        "      computationCounter=0\n",
        "      print()\n",
        "      print(\"Evaluating query...\")\n",
        "      for iterator in range(len(operations)):                                                    #Calling helper function to perform required operation\n",
        "          if(iterator == 0):\n",
        "              output2 = postingsListDictionary[tokensWithoutStopwords[0]]\n",
        "          print(\"Performing\",color.BOLD+operations[iterator]+color.END,\"operation between\",tokensWithoutStopwords[iterator],\"and\",tokensWithoutStopwords[iterator+1])  \n",
        "          output2,num = helperFunction(operations[iterator], output2, postingsListDictionary[tokensWithoutStopwords[iterator+1]])\n",
        "          computationCounter+=num\n",
        "      print()\n",
        "      print(color.BOLD+color.BLUE+\"Number of documents Matched:\"+color.DARKCYAN,len(output2),color.END)                   #Showing results\n",
        "      print()         \n",
        "      print(color.BOLD+color.BLUE+\"Number of comparisons required:\"+color.DARKCYAN,computationCounter)\n",
        "      print()\n",
        "      return output2\n",
        "    else:\n",
        "      return -1"
      ],
      "metadata": {
        "id": "8ql_nHJSl0sm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3LmaTKeplNf",
        "outputId": "4d5e28d5-a3cf-4f76-8490-906965964cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mEnter the Number of Queries :\u001b[0m\n",
            "1\n",
            "\u001b[1mEnter Query\u001b[0m\n",
            "lion stood moment\n",
            "\n",
            "\u001b[1m\u001b[94mGenerated tokens from your query: \u001b[36m ['lion', 'stood', 'moment'] \u001b[0m\n",
            "\u001b[1m\n",
            "Enter a list containing 2 operations:\u001b[0m\n",
            "[ OR, OR ]\n",
            "\n",
            "Evaluating query...\n",
            "Performing \u001b[1mOR\u001b[0m operation between lion and stood\n",
            "Performing \u001b[1mOR\u001b[0m operation between stood and moment\n",
            "\n",
            "\u001b[1m\u001b[94mNumber of documents Matched:\u001b[36m 209 \u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[94mNumber of comparisons required:\u001b[36m 305\n",
            "\n",
            "\u001b[1m\u001b[94mThe list of document names retrieved:\u001b[36m\n",
            "\n",
            "   bbq.txt\tgrommet.hum\tmowers.txt\t  hell.jok\tharmful.hum\t\n",
            " herb!.hum\t  hate.hum\tmothers.txt\t    hi.tec\t murph.jok\t\n",
            " conan.txt\t   icm.hum\thowlong.hum\t roach.asc\timprrisk.hum\t\n",
            "coyote.txt\thoroscop.jok\tinsanity.hum\tdirtword.txt\t bible.txt\t\n",
            "murphy.txt\tincarhel.hum\timpurmat.hum\tinsure.hum\tcartoon_.txt\t\n",
            "investi.hum\tinterv.hum\t  bless.bc\t  ivan.hum\tjaprap.hum\t\n",
            "jac&tuu.hum\t terms.hum\t       dym\t  test.jok\tkiller.hum\t\n",
            "kilsmur.hum\tkillself.hum\tkid_diet.txt\tlawsuniv.hum\ttestchri.txt\t\n",
            "blooprs1.asc\t test2.jok\tlbinter.hum\tmotrbike.jok\tmurphys.txt\t\n",
            "  simp.txt\t legal.hum\tludeinfo.txt\tlooser.hum\tlivnware.hum\t\n",
            "lifeimag.hum\tlif&love.hum\tlifeinfo.hum\toffice.txt\t   lll.hum\t\n",
            " llong.hum\tlobquad.hum\tluggage.hum\tlosers86.hum\tlozerzon.hum\t\n",
            "catstory.txt\tludeinfo.hum\tphunatdi.ana\tlosers84.hum\tlozeuser.hum\t\n",
            "manspace.hum\tmailfrag.hum\tmanilla.hum\tluzerzo2.hum\tm0dzmen.hum\t\n",
            "maecenas.hum\tmakebeer.hum\tmadscrib.hum\tmeinkamp.hum\tmarines.hum\t\n",
            "   pun.txt\t   mtm.hum\tmelodram.hum\t  mash.hum\tmiamadvi.hum\t\n",
            " miami.hum\t  memo.hum\tf_tang.txt\tmiranda.hum\treasons.txt\t\n",
            "mrscienc.hum\tmontpyth.hum\tmutate.hum\tmydaywss.hum\tcalvin.txt\t\n",
            "misery.hum\tmissheav.hum\tmyheart.hum\tnewconst.hum\tnaivewiz.hum\t\n",
            "  cars.txt\t  news.hum\tnewmex.hum\tboston.geog\t nurds.hum\t\n",
            "nysucks.hum\tcartoon.law\tnukeplay.hum\t  nuke.hum\tcartwb.son\t\n",
            " novel.hum\toilgluts.hum\to-ttalk.hum\toldeng.hum\todearakk.hum\t\n",
            "chunnel.txt\tohandre.hum\tonetoone.hum\t p-law.hum\tonetotwo.hum\t\n",
            "ozarks.hum\tookpik.hum\t  deep.txt\topinion.hum\tpassage.hum\t\n",
            "parabl.hum\tplaneget.hum\tpeatchp.hum\t phony.hum\tpizzawho.hum\t\n",
            "phorse.hum\tpoll2res.hum\tdevils.jok\tpopmusi.hum\tpopconc.hum\t\n",
            "dieter.txt\tpolicpig.hum\tprawblim.hum\tprayer.hum\tpsilaine.hum\t\n",
            "    dead-r\t quest.hum\tpro-fact.hum\tradiolaf.hum\trapmastr.hum\t\n",
            " raven.hum\tratings.hum\treagan.hum\tratspit.hum\tfuckyou2.txt\t\n",
            " donut.txt\trepair.hum\tresearch.hum\treport.hum\treddye.hum\t\n",
            "rentals.hum\tdocspeak.txt\tdrinkrul.jok\trockmus.hum\trocking.hum\t\n",
            "ripoffpc.hum\tdubltalk.jok\tt_zone.jok\t drunk.txt\t    insult\t\n",
            "corporat.txt\tcucumber.txt\tsmackjok.hum\t     jokes\tskippy.hum\t\n",
            "takenote.jok\tsmurfkil.hum\tshuttleb.hum\t    iqtest\tsocial.hum\t\n",
            "spydust.hum\t teens.txt\tsoleleer.hum\tterbear.txt\tsocecon.hum\t\n",
            "solviets.hum\tspider.hum\t stone.hum\tstandard.hum\ttermpoem.txt\t\n",
            "sungenu.hum\t top10.txt\tlanguag.jok\ttaping.hum\tt-shirt.hum\t\n",
            "  t-10.hum\tterrmcd'.hum\ttexican.dic\ttelevisi.hum\t     y.txt\t\n",
            "talebeat.hum\t  test.hum\ttimetr.hum\ttextgrap.hum\tthecube.hum\t\n",
            "tickmoon.hum\tterrnieg.hum\ttfpoems.hum\ttfepisod.hum\thecomes.jok\t\n",
            "lawyer.jok\t    let.go\t koans.txt\tthe_math.hel\ttoxcwast.hum\t\n",
            " turbo.hum\tcartoon.laws\tthesis.beh\ttruths.hum\t"
          ]
        }
      ],
      "source": [
        "#Driver code to start the execution\n",
        "try:\n",
        "  print(color.BOLD+'Enter the Number of Queries :'+color.END)\n",
        "  n = input()\n",
        "  totalTasks = int(n)\n",
        "  for iterator in range(totalTasks):\n",
        "    output1=findResultsNew()\n",
        "    if(output1 == -1):\n",
        "      raise Exception(\"\")\n",
        "    k=0\n",
        "    print(color.BOLD+color.BLUE+\"The list of document names retrieved:\"+color.DARKCYAN)\n",
        "    print()\n",
        "    for iteratorTwo in range(len(output1)):\n",
        "      print(\"{0:>10}\".format(nameId[iteratorTwo]),end=\"\\t\")\n",
        "      k+=1\n",
        "      if(k==5):\n",
        "        k=0\n",
        "        print()\n",
        "except:\n",
        "  print(color.BOLD+color.RED+'Invalid Input or No Documents'+color.RED)"
      ]
    }
  ]
}