{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR Assignment 1 Q2 Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import and Downloading Libraries**"
      ],
      "metadata": {
        "id": "6xrt0Uc9pU0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXak57TwbwmA",
        "outputId": "c6199270-1e1b-407a-8f22-a2be5f1b802c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "SdYgDIiqzxlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823f7381-380e-4d0c-c528-187591348e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk import PorterStemmer\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "id": "2CEim62UeRAE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Dataset Files**"
      ],
      "metadata": {
        "id": "omT41xMrpjDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasetFiles = os.listdir('/content/drive/MyDrive/Assignments/IR Assigment 1/Humor,Hist,Media,Food')"
      ],
      "metadata": {
        "id": "WR_MAQEm0KWX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating dictionaries**"
      ],
      "metadata": {
        "id": "rJrKM0NTpvQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating separate dictionaries for storing file names along with thier location path\n",
        "nameId={}\n",
        "locationId={} \n",
        "nameDictionary={} \n",
        "uniqueWords=[]         \n",
        "datasetCorpus = []\n",
        "frequencyDictionary={}           "
      ],
      "metadata": {
        "id": "uLlZliz6st_j"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating separate dictionaries for storing file names along with thier location path\n",
        "for iterator in range(len(datasetFiles)):\n",
        "    nameId[datasetFiles[iterator]] = iterator\n",
        "    nameDictionary[iterator] = datasetFiles[iterator] \n",
        "    locationId[iterator] = '/content/drive/MyDrive/Assignments/IR Assigment 1/Humor,Hist,Media,Food/' + datasetFiles[iterator]"
      ],
      "metadata": {
        "id": "atdnf9Da5icC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "LnZngQ6Pp4C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating dataset corpus...\")\n",
        "n = len(datasetFiles)\n",
        "for iterator in range(n):                                       \n",
        "    currentFileLocation=locationId[iterator]                                         #Reading current file\n",
        "    currentFile=open(currentFileLocation,'r',encoding='utf-8',errors='ignore')\n",
        "    currentFileText=currentFile.read()\n",
        "    textFetched=re.sub('[^a-zA-Z0-9]',' ',currentFileText)                           #Removing punctuations\n",
        "   \n",
        "    textFetched=textFetched.lower()                                                  #Converting the text of all files to lower case\n",
        "    \n",
        "    tokens=textFetched.split()\n",
        "   \n",
        "    tokens=[word for word in tokens if not word in stopwords.words('english')]       #Removing stopwords from the tokens\n",
        "    \n",
        "    for punct in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'':                           #Removing additional punctuations\n",
        "      for term in tokens:\n",
        "        term = term.replace(punct, '')\n",
        "\n",
        "    \n",
        "    for term in tokens:                                                              #Removing blank space tokens\n",
        "      term=term.strip()\n",
        "    textFetched=' '.join(tokens)\n",
        "    datasetCorpus.append(textFetched)\n",
        "print(\"Dataset corpus created successfully\")"
      ],
      "metadata": {
        "id": "OBXyafnt0Zx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02022ec2-6df6-48a8-f28f-c889b330031d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset corpus...\n",
            "Dataset corpus created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Index data structure implementation**"
      ],
      "metadata": {
        "id": "S8rLXW4xvCTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the frequency dictionary for storing the frequency of terms and generating the unique words\n",
        "n=len(datasetFiles)\n",
        "for iterator in range(n):\n",
        "    tokens=datasetCorpus[iterator].split()\n",
        "    for term in tokens:\n",
        "        if(term not in frequencyDictionary.keys()):\n",
        "            frequencyDictionary[term]={}\n",
        "            uniqueWords.append(term)"
      ],
      "metadata": {
        "id": "NM5ay4B_CSkH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stored the positions of words in the document\n",
        "for iterator in range(n):\n",
        "    tokens=datasetCorpus[iterator].split()\n",
        "    for term_pos,term in enumerate(tokens):\n",
        "        if iterator in frequencyDictionary[term].keys():\n",
        "            frequencyDictionary[term][iterator].append(term_pos)\n",
        "        else:\n",
        "            frequencyDictionary[term][iterator]=[term_pos]"
      ],
      "metadata": {
        "id": "V5vuX-hyEvbA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defined some functions for further use\n",
        "def prtstmt1(param1):\n",
        "  print(color.BOLD+color.BLUE+\"The number of documents retrieved after processing are: \"+color.END,param1)\n",
        "\n",
        "def prtstmt2(param2):\n",
        "  print()\n",
        "  print(color.BOLD+color.BLUE+\"The document ids retrieved are: \"+color.END,param2)\n",
        "\n",
        "def prtstmt3():\n",
        "  print()\n",
        "  print(color.BOLD+color.BLUE+\"The list of documents retrieved are: \"+color.END)\n",
        "\n",
        "def prtstmt4():\n",
        "  print()\n",
        "  print(color.BOLD+color.BLUE+'The query length needs to be less than 5\\n'+color.END)\n"
      ],
      "metadata": {
        "id": "FMATCe5JDx_g"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Input query and Driver function**"
      ],
      "metadata": {
        "id": "fo9LfZ3ngM04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the input query given by the user\n",
        "\n",
        "print(color.BOLD+'Please enter a query: '+color.END)\n",
        "inputQuery=input()\n",
        "textFetched=re.sub('[^a-zA-Z0-9]',' ',inputQuery)\n",
        "\n",
        "textFetched = textFetched.lower()                                                 #Converting text fetched to lower case\n",
        "\n",
        "t=textFetched.split()                                                             #Generating tokens \n",
        "\n",
        "t=[term for term in t if not term in stopwords.words('english')]                  #Removing stop words\n",
        "\n",
        "for punct in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'':                            #Removing punctuation marks\n",
        "  for term in t:\n",
        "    term = term.replace(punct, '')\n",
        "\n",
        "for term in t:                                                                    #Removing' blank space tokens\n",
        "  term=term.strip()\n",
        "print()\n",
        "print(color.BLUE+color.BOLD+'Generated tokens from your query : '+color.END,t)\n",
        "print()\n",
        "\n",
        "output=[]\n",
        "length_of_tokens=len(t)\n",
        "\n",
        "#if there are no tokens then the documents generated will be 0\n",
        "if(length_of_tokens==0):\n",
        "    length_of_output=len(output)\n",
        "    prtstmt1(length_of_output)\n",
        "    prtstmt2(output)\n",
        "    prtstmt3()\n",
        "    output1=[]\n",
        "    for iterator in output:\n",
        "        output1.append(nameDictionary[iterator])\n",
        "    print(output1)\n",
        "\n",
        "#if there is one token generated,then that token(1 word phrase) is searched within all documents\n",
        "try:\n",
        "  if(length_of_tokens==1):\n",
        "      if(t[0] in uniqueWords):\n",
        "          output=set(frequencyDictionary[t[0]].keys())\n",
        "      length_of_output = len(output)\n",
        "      prtstmt1(length_of_output)\n",
        "      prtstmt2(output)\n",
        "      prtstmt3()\n",
        "      output1=[]\n",
        "      for iterator in output:\n",
        "          output1.append(nameDictionary[iterator])\n",
        "      print(output1)\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)\n",
        "\n",
        "listoflist=[]    \n",
        "\n",
        "#if there are 2 tokens generated the function searches for 2 word phrases.\n",
        "try:\n",
        "  if(length_of_tokens==2):\n",
        "      l1=set(frequencyDictionary[t[0]].keys())\n",
        "      l2=set(frequencyDictionary[t[1]].keys())\n",
        "      doc=list(l1.intersection(l2))\n",
        "      output2=[]\n",
        "      for iterator in doc:\n",
        "        for a in frequencyDictionary[t[0]][iterator]:\n",
        "          for b in frequencyDictionary[t[1]][iterator]:\n",
        "              if(b-a==1):\n",
        "                  output2.append(iterator)        \n",
        "      output2=list(set(output2))\n",
        "      output=output2\n",
        "      length_of_output=len(output)\n",
        "      prtstmt1(length_of_output)\n",
        "      prtstmt2(output)\n",
        "      prtstmt3()\n",
        "      output1=[]\n",
        "      for iterator in output:\n",
        "          output1.append(nameDictionary[iterator])\n",
        "      print(output1)\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)\n",
        "\n",
        "\n",
        "#if there are 3 tokens generated the function searches for 3 word phrases.\n",
        "try:\n",
        "  if(length_of_tokens==3):\n",
        "      l1=set(frequencyDictionary[t[0]].keys())\n",
        "      l2=set(frequencyDictionary[t[1]].keys())\n",
        "      l3=set(frequencyDictionary[t[2]].keys())\n",
        "      lt=l1.intersection(l2)\n",
        "      doc=list(lt.intersection(l3))\n",
        "      output2=[]\n",
        "      for iterator in doc:\n",
        "        for a in frequencyDictionary[t[0]][iterator]:\n",
        "          for b in frequencyDictionary[t[1]][iterator]:\n",
        "              for c in frequencyDictionary[t[2]][iterator]:\n",
        "                  if(b-a==1 and c-b==1 and c-a==2):\n",
        "                      output2.append(iterator)               \n",
        "      output2=list(set(output2))\n",
        "      output=output2\n",
        "      length_of_output=len(output)\n",
        "      prtstmt1(length_of_output)\n",
        "      prtstmt2(output)\n",
        "      prtstmt3()\n",
        "      output1=[]\n",
        "      for iterator in output:\n",
        "          output1.append(nameDictionary[iterator])\n",
        "      print(output1)\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)\n",
        "\n",
        "#if there are 4 tokens generated the function searches for 4 word phrases.\n",
        "try:\n",
        "  if(length_of_tokens==4):\n",
        "      l1=set(frequencyDictionary[t[0]].keys())\n",
        "      l2=set(frequencyDictionary[t[1]].keys())\n",
        "      l3=set(frequencyDictionary[t[2]].keys())\n",
        "      l4=set(frequencyDictionary[t[3]].keys())\n",
        "      lt=l1.intersection(l2)\n",
        "      lt1=lt.intersection(l3)\n",
        "      doc=list(lt1.intersection(l4))\n",
        "      output2=[]\n",
        "      for iterator in doc:\n",
        "        for a in frequencyDictionary[t[0]][iterator]:\n",
        "          for b in frequencyDictionary[t[1]][iterator]:\n",
        "              for c in frequencyDictionary[t[2]][iterator]:\n",
        "                  for d in frequencyDictionary[t[3]][iterator]:\n",
        "                      if(b-a==1 and c-b==1 and d-c==1 and d-a==3):\n",
        "                          output2.append(iterator)        \n",
        "      output2=list(set(output2))\n",
        "      output=output2\n",
        "      length_of_output=len(output)\n",
        "      prtstmt1(length_of_output)\n",
        "      prtstmt2(output)\n",
        "      prtstmt3()\n",
        "      output1=[]\n",
        "      for iterator in output:\n",
        "          output1.append(nameDictionary[iterator])\n",
        "      print(output1)\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)\n",
        "\n",
        "#if there are 5 tokens generated the function searches for 5 word phrases.\n",
        "try:\n",
        "  if(length_of_tokens==5):\n",
        "      l1=set(frequencyDictionary[t[0]].keys())\n",
        "      l2=set(frequencyDictionary[t[1]].keys())\n",
        "      l3=set(frequencyDictionary[t[2]].keys())\n",
        "      l4=set(frequencyDictionary[t[3]].keys())\n",
        "      l5=set(frequencyDictionary[t[4]].keys())\n",
        "      lt=l1.intersection(l2)\n",
        "      lt1=lt.intersection(l3)\n",
        "      lt2=lt1.intersection(l4)\n",
        "      doc=list(lt2.intersection(l5))\n",
        "      output2=[]\n",
        "      for iterator in doc:\n",
        "        for a in frequencyDictionary[t[0]][iterator]:\n",
        "          for b in frequencyDictionary[t[1]][iterator]:\n",
        "              for c in frequencyDictionary[t[2]][iterator]:\n",
        "                  for d in frequencyDictionary[t[3]][iterator]:\n",
        "                      for e in frequencyDictionary[t[4]][iterator]:\n",
        "                          if(b-a==1 and c-b==1 and d-c==1 and e-d==1 and e-a==4):\n",
        "                              output2.append(iterator)        \n",
        "      output2=list(set(output2))\n",
        "      output=output2\n",
        "      length_of_output=len(output)\n",
        "      prtstmt1(length_of_output)\n",
        "      prtstmt2(output)\n",
        "      prtstmt3()\n",
        "      output1=[]\n",
        "      for iterator in output:\n",
        "          output1.append(nameDictionary[iterator])\n",
        "      print(output1)\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)\n",
        "\n",
        "#if there are more than 5 tokens generated then the query length exceeds 5 which is invalid\n",
        "try:\n",
        "  if(length_of_tokens>5):\n",
        "    prtstmt4()\n",
        "except:\n",
        "  print(color.RED+color.BOLD+'Invalid phrase,Try again'+color.END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwetyP6FxthG",
        "outputId": "9dcbd406-5382-4cbf-ea9c-577b58f5f4c6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mPlease enter a query: \u001b[0m\n",
            "lion\n",
            "\n",
            "\u001b[94m\u001b[1mGenerated tokens from your query : \u001b[0m ['lion']\n",
            "\n",
            "\u001b[1m\u001b[94mThe number of documents retrieved after processing are: \u001b[0m 23\n",
            "\n",
            "\u001b[1m\u001b[94mThe document ids retrieved are: \u001b[0m {662, 798, 553, 44, 941, 692, 55, 958, 324, 713, 1106, 600, 375, 624, 114, 755, 883, 117, 247, 507, 508, 637, 510}\n",
            "\n",
            "\u001b[1m\u001b[94mThe list of documents retrieved are: \u001b[0m\n",
            "['gd_gal.txt', 'three.txt', 'japantv.txt', 'murphys.txt', 'collected_quotes.txt', 'christop.int', 'llong.hum', 'computer.txt', 'puzzles.jok', 'filmgoof.txt', 'murphy_l.txt', 'lion.txt', 'lions.cat', 'booze1.fun', 'onetotwo.hum', 'dromes.txt', 'bitnet.txt', 'deep.txt', 'lion.jok', 'boneles2.txt', 'dthought.txt', 'stuf10.txt', 'tpquotes.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r7rjn8aRtqr3"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}